m1 = lm(airquality$Ozone, airquality$Solar.R)
lm(airquality$Ozone, airquality$Solar.R)
m1 = lm(airquality$Ozone ~ airquality$Solar.R)
termplot(m1)
summary(m1)
plot(airquality$Ozone, airquality$Solar.R)
plot(airquality$Solar.R,airquality$Ozone)
abline(m1)
predicted(m1)
predict(m1)
plot(z, se = TRUE, seWithMean=TRUE) #
plot(m1, se = TRUE, seWithMean=TRUE) #
confint(m1)
mtcars
m1 = lm(mpg ~ cyl, data=mtcars)
m1
m1$coefficients
m1$coefficients[0]
mtcars
for(i in names(mtcars)){
i
}
mtcars[i]
names(mtcars)
print(1)
for(i in names(mtcars)){
print(i)
}
print(i, mtcars$i)
for(i in names(mtcars)){
print(i, mtcars$i)
}
for(i in names(mtcars)){
print(mtcars$i)
}
print(mtcars[i])
for(i in names(mtcars)){
print(mtcars[i])
}
names(mtcars)
for(i in names(mtcars)){
for(i in names(mtcars)){
print(i,j)
}
}
names(mtcars)
for(i in names(mtcars)){
for(j in names(mtcars)){
print(i,j)
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print(i+","+j)
}
}
?print
for(i in names(mtcars)){
for(j in names(mtcars)){
print(c(i,j))
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print(c(i,j))
model = lm(mtcars$i~mtcars$j)
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print(c(i,j))
model = lm(mtcars[[i]]~mtcars[[j]])
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print(c(i,j))
model = lm(mtcars[[i]]~mtcars[[j]])
print(model$coefficients)
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print(c(i,j))
model = lm(mtcars[[i]]~mtcars[[j]])
print(model$coefficients)
print(model)
}
}
print("-----")
print("-----------")
for(i in names(mtcars)){
for(j in names(mtcars)){
print("-----------")
print(c(i,j))
model = lm(mtcars[[i]]~mtcars[[j]])
#print(model$coefficients)
print(model)
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print("-----------")
print(c(i,j))
model = lme(mtcars[[i]]~mtcars[[j]])
#print(model$coefficients)
print(model)
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print("-----------")
print(c(i,j))
model = lme(mtcars[[i]]~mtcars[[j]])
#print(model$coefficients)
summary(model)
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print("-----------")
print(c(i,j))
model = lm(mtcars[[i]]~mtcars[[j]])
#print(model$coefficients)
summary(model)
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
print("-----------")
print(c(i,j))
model = lm(mtcars[[i]]~mtcars[[j]])
#print(model$coefficients)
print(summary(model))
}
}
for(i in names(mtcars)){
for(j in names(mtcars)){
if (i!=j) {
print("-----------")
print(c(i,j))
model = lm(mtcars[[i]]~mtcars[[j]])
#print(model$coefficients)
print(summary(model))
}
}
}
?arr
?require
?t
?
;
?cbind
a <- c()
a
a[1] = 1
a
a[4] = 1
a
a.append(1)
a[1,1] = 1
a <- c(2.2)
a
topicList
cosineGen <- function(matrix){
lengthVec <- sqrt(rowSums(matrix * matrix))
tcrossprod(matrix) / (lengthVec %o% lengthVec)
}  #Function that generates the cosine between each row of a matrix.
calcEnt <- function(matrix){
workMatrix <- t(matrix) #transposes for division
a <- workMatrix / rowSums(workMatrix) #generates a probability matrix
b <- 1 + ((rowSums(a * log2(a), na.rm = T)) / log2(dim(matrix)[1])) #calculate entropy (1 + (sum of probability times log2 probability, divided by the total number of documents)).
workMatrix <- log(workMatrix[which(b > 0, arr.ind = T), ] + 1) #log normalizes frequency matrix and deletes 0 entropy ones.
workMatrix <- workMatrix * b[b > 0] #weight log normalized matrix by multiplying terms with entropy higher than 0 by its entropy.
return(t(workMatrix)) #returns original, non-transposed matrix.
} #calculates the entropy of each term of a matrix. Uses formula in Martin & Berry, 2007, "Mathematical foundations behind latent semantic analysis".
compareTheories <- function(matrix, cat){
##doc: takes matrix as the result of an svd(u). populates a pre-allocated list of every topic in external topicList (not generalized yet) with matrices of an "index" of similarity using each dimension (column) in matrix for each topic.
resultList <- lapply(lapply(1:8, matrix, data = 0, nrow = 8, ncol = dim(matrix)[2]), function(x){row.names(x) <- topicList; return(x)}) #pre-allocation of list
cosineAvgsList <- lapply(1:8, matrix, data = 0, nrow = 8, ncol = dim(matrix)[1]) #pre-allocation of second list
names(resultList) <- topicList #name for easy accessing theories
indexMatrix <- matrix(FALSE, nrow = dim(matrix)[1], ncol = 8, dimnames = list(1:dim(matrix)[1], topicList)) #pre-allocates logical matrix
for(topic in topicList){indexMatrix[,topic] <- cat[,2] == topic} #populates logical matrix with a logical mask reflecting catalog$id
docsByTop <- colSums(indexMatrix) #number of documents for each topic
n <- 1 #counter for each dimension
while(n <= dim(matrix)[2]){ #loops through dimensions
database <- matrix[, 1:n] #slices dimensions
if(n == 1){database <- cbind(database, 0)} #if it has only one dimension, then add a column of 0s to make cosineGen work
database <- cosineGen(database) #produces a x by x matrix of cosines between each paper.
database[is.na(database)] <- 0 #replaces NA with 0.
meanMatrix <- crossprod(indexMatrix, database) #produces a matrix with the sum of cosines of each paper with each of the topics
meanMatrix <- meanMatrix / docsByTop #produces a matrix with the mean cosine of each paper with each of the topics
cosineAvgsList[[n]] <- meanMatrix #stores the matrix of means in a list with n as index for dimensions used.
meanMatrix <- meanMatrix %*% indexMatrix #produces a vector with the sum of mean cosines for each topic against each topic in dimension n
meanMatrix <- t(meanMatrix) / docsByTop #produces a vector of the means of sums of mean cosines for each topic against each topic in dimension n.
for(topic in topicList){ #loops through topics to populate results of all cosines.
resultList[[topic]][, n] <- meanMatrix[topic,]
}
n = n + 1
}
returnList = list(resultList,cosineAvgsList) #makes list of lists with results and mean cosines
return(returnList) #returns everything
} #function for calculating cosines of the whole matrix. "matrix" is the result of an SVD; "cat" is the catalog to obtain topic information (in this case, catalog$id). Returns a list of two lists: [[1]] is all cosines by paper, [[2]] is a list of matrices of mean distance of each paper with each of the other topics. [[1]][n] and [[2]][n] are the different dimensions resulting from SVD.
plotTopicDiff <- function(topic, resultsList){
workTable <- as.data.frame(melt(resultsList[[1]][[topic]], varnames = c("topic", "dimension"), value.name = "cosine")) #long form for ggplot
plot <- ggplot(data = workTable, aes(x = dimension, y = cosine, color = topic, group = topic)) + theme_solarized(base_size = 14) + theme(axis.text = element_text(colour = "#586e75")) + labs(title = paste("Mean cosine of", topic, "papers with other theories and itself across dimensions")) + geom_line() + scale_colour_solarized("red") + geom_point(size = 0.7, shape = 3) + guides(colour = guide_legend(override.aes = list(size=3)))
print(plot)
} #function for plotting the mean distance of every topic with all other topics. "topic" is one of the topics of topicList; "resultsList" is the object that compareTheories() returns.
a = matrix(0)
a
a = matrix(2,2)
a
a = matrix(10,10)
a
a = matrix(10,10,1)
a
zeros(2,2)
zeros()
zeros()
mat.or.vec(3, 1)
mat.or.vec(10, 10)
?matrix
runif(10)
a <- mat.or.vec(10, 10,1)
a <- mat.or.vec(10, 10)
cosineGen(a)
a*1
a = a*1
a
a+1
rand()
randu()
randu(1)
a*randif(1)
a*runif(1)
a+runif(1)
runif(10,10)
runif([10,10])
runif(c(10,10))
c(10,10)
matrix(runif(36),6)
?matrix
matrix(runif(25),5,5)
seq(10)
seq(10) == seq(10)
seq(10) == c(10)
c(10)
type(seq(10))
summary(seq(10))
c(10,10,10)
rbind(c(2,3),c(4,5))
matrix(.25,3,5)
who
who()
whos
cov(seq(10),seq(10))
whos
mean(1792,2142,1462,2352,1079,1532,2077,2396)
plot(function(x)(df(x,df1=2,df2=150)),xlim=c(2,10))
?df
plot(function(x)(df(x,df1=4,df2=10)),xlim=c(2,10),lty="dashed",add=T)
1-pf(4,df=1,df2=150)
?pf
1-pf(2,df=1,df2=150)
plot(function(x)(df(x,df1=4,df2=5)),xlim=c(-2,10))
plot(function(x)(df(x,df1=4,df2=100)),xlim=c(2,10),add=T,lty="dashed")
lines(x=c(3.5,3.5),y=c(0,df(3.5,df1=4,df2=100)),col
="red",lwd=4)
1-pf(3.5,df1=4,df2=5)
1-pf(3.5,df1=4,df2=100)
females = rnorm(4, mean=200, sd=20)
males = rnorm(4, mean=120, sd=20)
kidz = rnorm(4, mean=380, sd=20)
pitchstudy = data.frame(females, males, kidz)
sum((females-mean(females))^2)
sum(females^2)-(sum(females)^2)/4
bigvector = c(females, males, kidz)
bigvector
females
CF = (sum(bigvector))^2/length(bigvector)
CF
total.ss = sum(bigvector^2)-CF
total
total.ss
between.ss = (sum(females)^2)/4 + (sum(males)^2)/4 + (sum(kidz)^2)/4 - CF
between.ss
within.ss
within.ss = total.ss - between.ss
within.ss
df.total = length(bigvector)-1
df.total
df.between = ncol(pitchstudy)-1
df.betwee
df.between
df.within = df.total-df.between
df.within
between.ms = between.ss/df.between # ms = mean squares
between.ms
within.ms = within.ss/df.within
within.ms
F.value = between.ms/within.ms
F.value
1-pf(F.value,2,9)
groups = c(rep("female",4),rep("male",4),rep("kidz",4))
groups = c(rep("female",4),rep("male",4),rep("kidz",4))
groups
pitchstudy = data.frame(c(1:12),groups,bigvector)
pitchstudy
colnames(pitchstudy) = c("subjects","groups","bigvector")
pitchstudy
summary(aov(bigvector ~ groups + Error(subjects), data=pitchstudy))
summary(pitchstudy)
class(pitchstudy$subjects)
pitchstudy$subjects= as.factor(pitchstudy$subjects) # recode from integer to factor
class(pitchstudy$subjects)
summary(aov(bigvector ~ groups + Error(subjects), data=pitchstudy))
install.packages(c("readbulk","mousetrap"))
package?mousetrap::bezier()
package?mousetrap
library(mousetrap)
raw_data <- KH2017_raw
summary(raw_data)
plot(raw_data)
View(raw_data)
View(raw_data)
str(raw_data)
mt_remap_symmetric(raw_data)
mt_data < mt_import_mousetrap(raw_data)
mt_data <- mt_import_mousetrap(raw_data)
mt_remap_symmetric(mt_data)
mt_data <- mt_remap_symmetric(mt_data)
plot(mt_data)
mt_plot(mt_data)
mt_data <- mt_aggregate(mt_data)
mt_data <- mt_align_start(mt_data)
mt_plot(mt_data)
mt_animate(mt_data)
mt_derivatives(mt_data)
mt_sample_entropy(mt_data)
mt_measures(mt_data)
mt_time_normalize(mt_data)
mt_sample_entropy(mt_data)
mt_plot_aggregate(mt_data)
mt_plot_riverbed(mt_data)
tn_trajectory <- mt_time_normalize(mt_data)
mt_sample_entropy(mt_data)
tn_trajectories <- mt_time_normalize(mt_data)
mt_sample_entropy(mt_data)
tn_trajectories <- mt_time_normalize(mt_data)
mt_sample_entropy(mt_data)
mt_plot_riverbed(mt_data)
mt_plot_add_rect()
mt_plot_add_rect(mt_data)
mt_check_bimodality(mt_data)
ls
mt_animate(mt_data) # Not working
mt_heatmap(mt_data)
mt_diffmap(mt_data)
mt_diffmap(mt_data, condition= Condition)
mt_diffmap(mt_data)
spatialize <- mt_spatialize(mt_data)
mt_cluster(spatialize)
clusters <- mt_cluster(spatialize)
mt_plot(clusters)
clusters <- mt_cluster(spatialize, n_cluster = 5)
mt_plot(clusters)
mt_plot(clusters, use2="clustering")
1 - pf(7.602, df=7, df2=1003)
options(scipen = 999)
1 - pf(7.602, df=7, df2=1003)
0.000000005894523 < 0.00000001
females = rnorm(4, mean=200, sd=20)
males = rnorm(4, mean=120, sd=20)
kidz = rnorm(4, mean=120, sd=20)
pitchstudy = data.frame(females, males, kidz)
pitchstudy = data.frame(females, males, kidz)
bigvector = c(females, males, kidz) #total vector
CF = (sum(bigvector))^2/length(bigvector) #correction factor
total.ss = sum(bigvector^2)-CF #denominator of equation for variance in total
between.ss = (sum(females)^2)/4 + (sum(males)^2)/4 + (sum(kidz)^2)/4 - CF
within.ss = total.ss - between.ss
df.total = length(bigvector)-1
df.between = ncol(pitchstudy)-1
df.within = df.total-df.between
between.ms = between.ss/df.between
within.ms = within.ss/df.within
F.value = between.ms/within.ms
females = rnorm(4, mean=200, sd=20)
males = rnorm(4, mean=120, sd=20)
kidz = rnorm(4, mean=380, sd=20)
pitchstudy = data.frame(females, males, kidz)
sum((females-mean(females))^2)
sum(females^2)-(sum(females)^2)/4
bigvector = c(females, males, kidz)
bigvector
install.packages("bibliometrix")
install.packages("bibliometrix")
updater()
update('r')
data = [66.51, 54.3, 63.93, 60.45, 69.23, 55.93, 53.44, 68.65, 56.87, 51.06, 45.54, 56.24, 53.79, 50.64, 60.56, 64.79, 62.64, 70.37, 65.22, 50.64, 57.29, 68.65, 57.83, 57.88, 64.79, 58.36, 53.21, 18.99, 59.97, 61.36, 61.79, 61.51, 69.29, 59.21, 71.22, 67.36, 40.92, 66.08, 64.26, 66.93, 46.93, 59.37, 51.92, 51.64, 67.36, 56.64, 65.22, 71.22, 67.36, 62.17, 67.94, 62.64, 71.22, 57.5, 57.62, 64.74, 63.23, 69.94, 72.51, 52.35, 0.0,]
data = [66.51, 54.3, 63.93, 60.45, 69.23, 55.93, 53.44, 68.65, 56.87, 51.06, 45.54, 56.24, 53.79, 50.64, 60.56, 64.79, 62.64, 70.37, 65.22, 50.64, 57.29, 68.65, 57.83, 57.88, 64.79, 58.36, 53.21, 18.99, 59.97, 61.36, 61.79, 61.51, 69.29, 59.21, 71.22, 67.36, 40.92, 66.08, 64.26, 66.93, 46.93, 59.37, 51.92, 51.64, 67.36, 56.64, 65.22, 71.22, 67.36, 62.17, 67.94, 62.64, 71.22, 57.5, 57.62, 64.74, 63.23, 69.94, 72.51, 52.35, 0.0]
data = c(66.51, 54.3, 63.93, 60.45, 69.23, 55.93, 53.44, 68.65, 56.87, 51.06, 45.54, 56.24, 53.79, 50.64, 60.56, 64.79, 62.64, 70.37, 65.22, 50.64, 57.29, 68.65, 57.83, 57.88, 64.79, 58.36, 53.21, 18.99, 59.97, 61.36, 61.79, 61.51, 69.29, 59.21, 71.22, 67.36, 40.92, 66.08, 64.26, 66.93, 46.93, 59.37, 51.92, 51.64, 67.36, 56.64, 65.22, 71.22, 67.36, 62.17, 67.94, 62.64, 71.22, 57.5, 57.62, 64.74, 63.23, 69.94, 72.51, 52.35, 0.0)
mean(data)
data = c(92.71, 67.4, 90.13, 86.65, 95.43, 82.14, 66.54, 94.85, 69.98, 77.27, 71.75, 82.44, 79.99, 76.84, 95.5, 90.99, 88.85, 96.57, 91.42, 76.84, 91.21, 94.85, 84.03, 84.08, 90.99, 84.56, 79.41, 52.68, 86.18, 87.56, 87.99, 87.71, 95.5, 85.42, 97.43, 93.57, 67.13, 92.28, 90.47, 93.14, 73.13, 85.57, 78.13, 77.85, 93.57, 82.84, 91.42, 97.43, 93.57, 88.37, 94.15, 88.85, 97.43, 83.7, 91.64, 90.94, 89.43, 96.14, 98.71, 78.55,)
data = c(92.71, 67.4, 90.13, 86.65, 95.43, 82.14, 66.54, 94.85, 69.98, 77.27, 71.75, 82.44, 79.99, 76.84, 95.5, 90.99, 88.85, 96.57, 91.42, 76.84, 91.21, 94.85, 84.03, 84.08, 90.99, 84.56, 79.41, 52.68, 86.18, 87.56, 87.99, 87.71, 95.5, 85.42, 97.43, 93.57, 67.13, 92.28, 90.47, 93.14, 73.13, 85.57, 78.13, 77.85, 93.57, 82.84, 91.42, 97.43, 93.57, 88.37, 94.15, 88.85, 97.43, 83.7, 91.64, 90.94, 89.43, 96.14, 98.71, 78.55)
mean(data)
R.version()
R.version.string
updateR()
install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR()
updateR(admin_password = 'htext2')
htext2
install.packages(as.vector(needed_packages))
R.version()
R.version.string
updateR(admin_password = 'htext2')
install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
library(updateR)
updateR(admin_password = 'htext2')
install.packages(as.vector(needed_packages))
data("mtcars")
am.glm = glm(formula=am ~ hp + wt,
data=mtcars,
family=binomial)
summarize(am.glm)
am.glm = glm(formula=am ~ hp + wt,
data=mtcars,
family=binomial)
summary(am.glm)
summary(am.glm)
setwd('/Users/jyoshimi/ipython/scientometrics/v2')
require('bibliometrix')
require('stringr')
files <- readFiles('savedref_8.txt', 'savedref_7.txt', 'savedref_6.txt',
'savedref_5.txt', 'savedref_4.txt', 'savedref_3.txt',
'savedref_2.txt', 'savedref_1.txt')
# Convert files to dataframe. isi is web of science
df <- convert2df(files,dbsource = "isi", format = "plaintext")
# Create a new data frame, that is exactly like DF but includes a new
# column with a new "authors" meta-tag (a meta-tag is a combination of basic tags)
# All this does is take the CR tag, and strip out the authors (open a raw file to grok further)
df <- metaTagExtraction(M = df, Field = "CR_AU")
# Remove rows which don't have cross-reffed authors
df = df[which(df$CR_AU != "NA"),]
# Create the co-occurrence matrix.  Each row is an author of the downloaded docs,
# Each column is an author or authors (for multi-authored works) that the original author cites.
# Note that the same cited author can occur in multiple columns
cocm <- cocMatrix(df, Field = "CR_AU", type = "matrix", sep = ";")
# Save the matrix in case I want to do python stuff on it
write.csv(cocm,file = "cocm.csv",sep = ",")
# Take all authors from original data frame.  Semicolon separates multiple authors.
z <- strsplit(df$AU,";")
authors <- c()
for(i in 1:length(z)){
authors <- c(authors, z[[i]][1])
}
# Convert cocm matrix to a dataframe
cocm <- as.data.frame(cocm)
# Add an extra column with the first author of a document
cocm$author <- authors
# Alphabetically orders documents by row and by colums.  Rows and columns alphabetically ordered
cocmOrdered <- y[order(row.names(cocm)),]
# Alphabetically orders documents by row and by colums.  Rows and columns alphabetically ordered
cocmOrdered <- cocm[order(row.names(cocm)),]
cocmOrdered <- cocmOrdered[, order(colnames(cocmOrdered))]
# Save the matrix in case I want to do python stuff on it
write.csv(cocmOrdered,file = "cocmOrdered.csv",sep = ",")
print(authors)
# Clean up authors
# print(authors)
# removes all second initials (marion j l -> marion j)
cocmOrdered$author <- str_replace(string = cocmOrdered$author, replacement = "\\1", pattern = "^([A-Z]+[[:space:]][A-Z]{1})([[:space:]])([A-Z]{1})")
# remove all full first names (marion jeanluc -> marion j)
cocmOrdered$author <- str_replace(string = cocmOrdered$author, pattern = "(^[A-Z]+[[:space:]][A-Z]{1})([A-Z]+)", replacement = "\\1")
# Merge all rows that share an author
cocmPrime <- aggregate(. ~ author, FUN = sum, data = cocmOrdered)
row.names(cocmPrime) <- cocmPrime$author
# Removes now unneeded author column
cocmPrime <- cocmPrime[, which(colnames(cocmPrime) != "author")]
# Do the same kind of thing for columns. Merge columns with same author.
# Method here was to transpose and use the method above
#clean up columns now TODO: Change names to cocm stuff
yPrimeReversed <- as.data.frame(t(cocmPrime))
yPrimeReversed$authors <- row.names(yPrimeReversed)
# Add a new column with row-names
yPrimeReversed$authors <- row.names(yPrimeReversed)
# Regex away on the columns this time
yPrimeReversed$authors <- str_replace(string = yPrimeReversed$authors, pattern = "ARIST[A-Z]*", replacement = "ARISTOTLE")
yPrimeReversed$authors <- str_replace_all(string = yPrimeReversed$authors, pattern = "HUSS*[EA]R?L?[^YS]([[:space:]]\\w*)*", replacement = "HUSSERL E")
yPrimeReversed$authors <- str_replace(string = yPrimeReversed$authors, pattern = "(^[A-Z]+[[:space:]][A-Z]{1})([A-Z]+)", replacement = "\\1") # remove all full first names (marion jeanluc -> marion j)
yPrimeReversed$authors <- str_replace(string = yPrimeReversed$authors, replacement = "\\1", pattern = "(^[A-Z]+[[:space:]][A-Z]{1})([[:space:]])([A-Z]{1})([[:space:]]*[A-Z]*)") # removes all second initials (marion j l -> marion j)
# This is where the merge happens
yPrimeReversedPrime <- aggregate(. ~ authors, FUN = sum, data = yPrimeReversed)
# Write the final matrix
write.csv(smallerFinalY, file = "cleaned_matrix.csv", row.names = T, col.names = T)
row.names(yPrimeReversedPrime) <- yPrimeReversedPrime$authors
yPrimeReversedPrime <- yPrimeReversedPrime[, which(colnames(yPrimeReversedPrime) != "authors")]
# Transpose back
finalY <- t(yPrimeReversedPrime)
# Get rid of authors cited less than 5 times and who cited someone less than five times
# (includes only authors with more than 5 citations from authors with total citations to other authors larger than 5.)
smallerFinalY <- finalY[, which(colSums(finalY) > 5, arr.ind = T)]
smallerFinalY <- smallerFinalY[which(rowSums(smallerFinalY) > 5, arr.ind =  T),]
# Write the final matrix
write.csv(smallerFinalY, file = "cleaned_matrix.csv", row.names = T, col.names = T)
